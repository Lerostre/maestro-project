import pandas as pd
import numpy as np
import ast
from functools import reduce
from tabulate import tabulate

custom_diff = lambda z: reduce(lambda x, y: abs(x - y), z)


def consecutive(x):
    if len(x) == 2:
        # если два, то вырождается
        return 1 if x[-1] - x[0] == 1 else 0
    if len(x) > 2:
        return 1 if (x[0] < x[-1]) and (len(x)*(x[0]+x[-1])/2 == sum(x)) else 0


consecutive([2, 1])


def search(request, df=sents):
    sub_req = request.split()

    el_chars = {}
    for element in sub_req:
        with_pos = False
        if element[0] == '"':
            mode = "lemmas"
        elif element[0].isascii():
            mode = "poss"
        else:
            mode = "tokens"
        if "+" in element:
            with_pos = True
        print(element, mode)
        el_chars.update({element: word_search(element=element,
                                              sent_df=df, mode=mode,
                                              with_pos=with_pos)})
        
    if len(el_chars) == 1:
        sent, word_indices = list(el_chars[request].keys()), list(el_chars[request].values())
        print_out(sent, word_indices)

    else:
        common_sents = list(reduce(lambda x, y: x & y.keys(), el_chars.values()))

        for sent in common_sents:
            # беру разницу элементов массива, она должна быть равна длине массива -1
            listik = []
            for key in el_chars:
                listik.append(el_chars[key][sent])
            grid = np.array(np.meshgrid(*listik)).T.reshape(-1, len(el_chars))
            print(grid)
            diffs = np.apply_along_axis(consecutive, arr=grid, axis=1)
            print(diffs)
            word_indices = diffs[np.where(diffs == 1)]
            if word_indices.size > 0:
                print_out(sent, word_indices)


def print_out(sent_indice, word_indice, df=meta):
    if sent_indice:
        row = df.iloc[sent_indice].reset_index()
        np.apply_along_axis(lambda a: print(a), arr=row, axis=1)
    #print(row.sentence)
    #print(row.title)
    #print(row.date)
    print("")


def word_search(element, sent_df=sents, word_df=words, mode="tokens", with_pos=False):
    
    if mode == "lemmas":
        element = element[1:-1]

    if with_pos:
        word, pos = element.split("+")
        word_indices = word_search(word, sent_df, word_df, mode)
        pos_indices = word_search(pos, sent_df, word_df, mode="poss")
        indices = word_indices.keys() & pos_indices.keys()
        morph_indices = []
        for sent in indices:
            morph_indices.append(list(set(word_indices[sent]) & set(pos_indices[sent])))

    else:
        v = np.vectorize(lambda x: element in x)
        indices = np.where(v(sent_df[mode]) == True)[0]
        row = sent_df.iloc[indices]
        morph_indices = row[mode].apply(lambda x: x[element])
        if with_pos:
        #print(row[mode].apply(lambda x: x[word]))  # индексы находятся списками
            morph = word_df.iloc[morph_indices.sum()]
            u = np.vectorize(lambda x: x == pos)

    if not any(morph_indices):
        indices = []

    return dict(zip(indices, morph_indices))


meta = pd.read_csv("sents_meta.csv")
sents = pd.read_csv("parsed_sents.csv")
sents = sents.applymap(lambda x: ast.literal_eval(x))
words = pd.read_csv("morph_info.csv")


sents.iloc[5346].lemmas


meta.to_csv("sents_meta.csv", index=0)
sents.to_csv("whyyyyyyy.csv", index=0)
words.to_csv("morph_info.csv", index=0)


word_search('знать+NOUN', sent_df=sents, word_df=words, mode="tokens", with_pos=True)


https://stackoverflow.com/questions/24893977/whats-the-best-way-to-regex-replace-a-string-in-python-but-keep-its-case


import re
def replace_keep_case(word, replacement, text):
    def func(match):
        g = match.group()
        if g.islower(): return replacement.lower()
        if g.istitle(): return replacement.title()
        if g.isupper(): return replacement.upper()
        return replacement      
    return re.sub(word, func, text, flags=re.I)


text = "Eggs with eggs, bacon and spam are good for breakfast... EGGS!"
replace_keep_case("eggs", "spam", text)


element = "да"

v = np.vectorize(lambda x: element in x)
indices = np.where(v(sents["tokens"]) == True)[0]


sents.iloc[2131].tokens


sents.iloc[2131].poss["NOUN"] = [44028, 44030, 44033, 44034, 44036, 44042, 44044]
sents.iloc[2131].poss["VERB"] = [44032, 44041]


words.iloc[44028].tag


words.iloc[44030].pos = "NOUN"
words.iloc[44030].tag = "Case=Nom|Gender=Fem|Number=Sing"


sents.ro_csv(")


consecutive([441796, 441799])


(441796+441799)


get_ipython().run_cell_magic("time", "", """
query = "ты был"

results = search(query, sents, words, meta)""")


query = "вот как"

results = search(query, sents, words, meta)


words.iloc[1]


words[["token", "tag"]].values


i = 0
def get_range(obj):
    if obj:
        return range(min(obj)[0], max(obj)[0]+1)
    else:
        pass


i = 0
def get_capitals(obj):
    if obj:
        return int(min(obj)[0])
    else:
        pass


min(sents.tokens[0].values())[0]


capitals = sents["tokens"].apply(lambda x: get_capitals(x.values()))


capitals = capitals.dropna().astype(int)


capitalized = words.iloc[capitals]
capitalized = capitalized.token.apply(lambda x: x.capitalize()).rename("caps")


words["caps"] = pd.concat([words.drop(capitals).token, capitalized])


capitalized = capitalized.token.apply(lambda x: x.capitalize()).rename("caps")


capitalized = capitalized.token.apply(lambda x: x.capitalize()).remame("caps")


a = sents["tokens"].apply(lambda x: get_range(x.values())).rename("ranges")
sents_ranged = pd.concat([sents, a], axis=1)


da = lambda x: f'<div class="tooltip2">{x[0]}<span class="tooltiptext2">{x[1]}</span></div>'

tooltips = np.apply_along_axis(da, arr=words[["caps", "tag"]].values, axis=1)


tooltipped_words = pd.concat([words, pd.DataFrame({"tooltips": tooltips})], axis=1)


def ramble_tips(tokens_range, meta_tooltips=tooltipped_words.tooltips):
    if tokens_range:
        return " ".join(meta_tooltips[token_idx] for token_idx in tokens_range)
    else:
        pass


meta["tooltipped"] = sents_ranged.ranges.apply(ramble_tips).rename("tooltipped")


meta.tooltipped[2]


sents_tooltipped = pd.concat([meta, sents_ranged.ranges.apply(ramble_tips).rename("tooltipped")], axis=1)


meta.to_csv("meta_tooltipped.csv", index=0)


tooltipped_words.iloc[:, [0, -1]].to_csv("tooltips.csv", index=0)


from markupsafe import Markup
value = Markup('<strong>The HTML String</strong>')


tooltipped_words.to_csv("tooltipped_words.csv", index=0)


tooltip_wrap(1, word_df=words, meta_df=meta)


len(list(zip(results[0::2], results[1::2]))[::2])


a = [[0], [1], [2], [3]]
flat_list = [item for sublist in a for item in sublist]


def flatten(x):
    l = list(x.values())
    return [item for sublist in l for item in sublist]


sent_dict = sents.lemmas.apply(flatten).explode().drop_duplicates()


new_words = pd.concat([words, pd.Series(list(sent_dict.index))], axis=1)


word_search("NOUN", mode="poss", sent_df=sents, word_df=words)


get_ipython().run_cell_magic("timeit", "", """
element = "я"

v = np.vectorize(lambda x: element in x)
indices = np.where(v(sents["tokens"]) == True)[0]
row = sents.iloc[indices]
morph_indices = row["tokens"].apply(lambda x: x[element])""")


get_ipython().run_cell_magic("timeit", "", """
temp = new_words[new_words["token"] == element]
morph_indices1 = list(temp.index)
indices1 = temp[0].values""")


(indices == indices1).all()


import numpy as np
import re
from functools import reduce
import pymorphy2

morph = pymorphy2.MorphAnalyzer()


def consecutive(x):
    if len(x) == 2:
        # если два, то вырождается
        return 1 if x[-1] - x[0] == 1 else 0
    if len(x) > 2:
        return 1 if (x[0] < x[-1]) and (len(x)*(x[0]+x[-1])/2 == sum(x)) else 0

    
def search(request, sent_df, word_df, meta_df):
    sub_req = request.split()
    info = []

    el_chars = {}
    for element in sub_req:
        with_pos = False
        if element[0] == '"':
            mode = "tokens"
        elif element[0].isascii():
            mode = "poss"
        else:
            mode = "lemmas"
        if "+" in element:
            with_pos = True
        print(element, mode)
        el_chars.update({element: word_search(element=element,
                                              sent_df=sent_df,
                                              word_df=word_df,
                                              mode=mode,
                                              with_pos=with_pos)})
        
    if len(el_chars) == 1:
        sents, word_indices = list(el_chars[request].keys()), list(el_chars[request].values())
        for sent in sents:
            new_info = extract_info(sent, word_indices, meta_df, word_df, 1)
            if new_info:
                info.append(new_info)

    else:
        common_sents = list(reduce(lambda x, y: x & y.keys(), el_chars.values()))

        for sent in common_sents:
            listik = []
            for key in el_chars:
                listik.append(el_chars[key][sent])
            grid = np.array(np.meshgrid(*listik)).T.reshape(-1, len(el_chars))
            diffs = np.apply_along_axis(consecutive, arr=grid, axis=1)
            word_indices = diffs[np.where(diffs == 1)]
            if word_indices.size > 0:
                info.append(extract_info(sent, word_indices, meta_df, word_df))

    return info


def replace_keep_case(word, replacement, text):
    def func(match):
        g = match.group()
        if g.islower(): return replacement.lower()
        if g.istitle(): return replacement.title()
        if g.isupper(): return replacement.upper()
        return replacement      
    return re.sub(rf"\b{word}\b", func, text, flags=re.I)


def highlight(sentence, string):
    sent_check = sentence.lower()
    
    if not string[0].isascii() and f"<mark>{string}</mark>".lower() not in sent_check:
        return replace_keep_case(string, f'<mark>{string}</mark>', sentence)
    else:
        return sentence


def highlight2(sentence, string):
    red = "\033[31m"
    nul = "\033[0m"
    sentence = re.sub(
        r"\b({})\b".format(re.escape(string)),
        r"{}\1{}".format(red, nul),
        sentence,
        flags=re.I,
    )
    return sentence


def extract_info(sent_indice, word_indice, meta_df, word_df, num=2):
    if sent_indice:
        row = meta_df.iloc[sent_indice]
        sent = row.sentence
        if num == 1:
            words = [word_df.iloc[item].token for sublist in word_indice for item in sublist]
            for word in words:
                sent = highlight(sent, word)
        #print(word_df.iloc[word_indice].pos)
        return {"index": str(sent_indice), "sents": sent,
               "titles": row.title, "dates": row.date}
    else:
        return None
    
    
def word_search(element, sent_df, word_df, mode="tokens", with_pos=False, is_normal=False):
    
    res = {}
    
    print(element, mode, with_pos)

    if mode == "lemmas" and not is_normal:
        elements = query_parse(element)
        for el in elements:
            res.update(word_search(el, sent_df, word_df, mode, with_pos, is_normal=True))
        return res

    elif mode == "tokens":
        element = element[1:-1]

    if with_pos:
        indices = []
        word, pos = element.split("+")
        word_indices = word_search(word, sent_df, word_df, mode)
        pos_indices = word_search(pos.upper(), sent_df, word_df, mode="poss")
        indices_candidates = word_indices.keys() & pos_indices.keys()
        morph_indices = []
        for sent in indices_candidates:
            candidate = list(set(word_indices[sent]) & set(pos_indices[sent]))
            if candidate:
                indices.append(sent)
                morph_indices.append(candidate)

    else:
        v = np.vectorize(lambda x: element in x)
        indices = np.where(v(sent_df[mode]) == True)[0]
        row = sent_df.iloc[indices]
        morph_indices = row[mode].apply(lambda x: x[element])

    if not any(morph_indices):
        indices = []

    return dict(zip(indices, morph_indices))


def query_parse(word):
    variants = []

    for i in morph.parse(word):
        form = i.normal_form
        if form not in variants:
            variants.append(form)

    return variants


def morphy_converter(x):
    converter = {"ADJF": "ADJ", "ADJS": "ADJ", "COMP": "ADJ",
                "INFN": "VERB", "PRTF": "VERB", "PRTS": "VERB", "GRND": "VERB",
                "NUMR": "NUM", "ADVB": "ADV", "PREP": "ADP", "PTCL": "PART"}
    return converter[x] if x in converter else x


def highlight(sentence, string):
    # не работает, если слово есть в предложении, но без чего-то
    words = " ".join(x for x in string.split("+")).split()
    string_length = len(words)
    
    sent_words = sentence.split()
    sent_idx = sent_words.index(words[0])
    words_to_change = " ".join(sent_words[sent_idx+x] for x in range(string_length))
    
    print(words_to_change)
    
    return replace_keep_case(words_to_change, f'<mark>{words_to_change}</mark>', sentence)


res = word_search("ели", sents, words, mode="lemmas")


word_search("ель", sents, words, mode="lemmas")


word_search("есть", sents, words, mode="lemmas")


y = {"да" : 1}
x = {"нет": 2}
z = {"ах": 2}
a = {"sх": 3}

x.update(y)


replace_with_case(sent, "да")


query = 'молодой "красивый" ADJ'

results = search(query, sents, words, meta)


results


word_search("ель", sent_df, word_df, mode)


def highlight(sentence, string):
    print(string[0])
    if string[0].isascii():
        
    else:
        return replace_keep_case(string, f'<mark>{string}</mark>', sentence)


import spacy
from spacy import displacy


import pymorphy2
morph = pymorphy2.MorphAnalyzer()


def query_parse(word):
    variants = []

    for i in morph.parse(word):
        form = i.normal_form
        if form not in variants:
            variants.append(form)

    return variants


def morphy_converter(x):
    converter = {"ADJF": "ADJ", "ADJS": "ADJ", "COMP": "ADJ",
                "INFN": "VERB", "PRTF": "VERB", "PRTS": "VERB", "GRND": "VERB",
                "NUMR": "NUM", "ADVB": "ADV", "PREP": "ADP", "PTCL": "PART"}
    return converter[x] if x in converter else x


query_parse("ели")


def morphy_converter(x):
    converter = {"ADJF": "ADJ", "ADJS": "ADJ", "COMP": "ADJ",
                "INFN": "VERB", "PRTF": "VERB", "PRTS": "VERB", "GRND": "VERB",
                "NUMR": "NUM", "ADVB": "ADV", "PREP": "ADP", "PTCL": "PART"}
    return converter[x] if x in converter else x


predictor.predict(["ели"])[0].normal_form


from rnnmorph.predictor import RNNMorphPredictor

predictor = RNNMorphPredictor(language="ru")


words = words.drop(list(words[(words.token == "е") | (words.token == "п")].index), axis=0)


words[(words.token == "п")].pos.value_counts()


words = words.reset_index(drop=True)


def highlight(sentence, string):
    if string[0] == '"':
        string = string[1:-1]
    if string[0].isascii():
        return sentence
    else:
        return replace_keep_case(string, f'<mark>{string}</mark>', sentence)


highlight("Я хочу пиццы!", '"хочу"')


a = '"хочу"'
a[1:-1]


я был
хайлайты


words = words.token


words = pd.DataFrame(words)


sents
