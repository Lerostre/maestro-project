# Первый научный корпус русского языка

Здесь покоится всё то, что я успел наклепать в рамках проектной работы по курсу "Автоматическая обработка естественного языка". Проект предполагал создание jupyter-notebook'а (лежит в репозитории для дз, хотя не уверен, насколько он читаем), либо же создание веб-приложения, что я с гордостью сюда и загружаю. Задача заключалась в собрании и разметке корпуса и реализации возможности поиска на его материале. Готовое приложение находится по этой ссылке - http://lerostre.pythonanywhere.com. Засим, констатирую, что задание выполнено. [Презентация здесь](https://rr.noordstar.me/--66371436). Ниже подробности, но не все, что-то расписано на сайте, что-то в `web_search.py`:

### Материалы корпуса
Основанием для корпуса послужило творчество знаменитого писателя, светоча современной исторической науки, талантливого театрального режиссёра и актёра, ревностного деятеля искусства, музыканта, обличителя всех нынешних нравственных устоев, философа с чувствующей душевной организацией и просто хорошего человека - Евгения Николаевича Понасенкова. В файле `source_gathering.ipynb` находится единственная нормальная тетрадка с комментариями, где детально описан процесс. Я использовал данные из его личного блога - https://www.mk.ru/blogs/blog-evgeniya-ponasenkova.html, посты со страницы вк - https://vk.com/evgenyponasenkov, а также из текстов двух его больших книг. К сожалению, без его на то резрешения, надеюсь, он меня не засудит.    

Тексты были размечены при помощи `tesseract`. Качество его работы довольно высокое, распознан в том числе и текст на иностранном языке, но ошибки всё равно периодически встречаются. Хотя автору кажется, что их не так много, чтобы испортить впечатление от пользования корпусом. Всего было собрано почти 30000 предложений и 500000 слов

### Разметка корпуса
Токенизатор, использовавшийся при разметке корпуса был самым обыкновенным - брались слова без цифр и знаков препинания. Надо сказать, что это была ошибка, и по-хорошему вот эту часть нужно обработать заново, но почему этого делать очень не хочется - далее. Из-за такого подхода в тексте неправильно группируются, например, аббревиатуры. Предложения в целом группируются нормально, однако исходный текст также следовало бы обработать. В частности, Е.Н. Понасенков в один момент своей монографии начинает беспорядочно перечислять все используемые им источники, они никакой полезной информации не несут, текст нужно было бы предварительно почистить.

Поскольку размечать нужно все предложения целиком, для этого я брал решения, которые смотрят на контекст. Для задач поиска мне были интересны прежде всего наиболее быстрые инструменты, из которых лучшим является, наверное, `pymorphy`. Я взял нечто похожее - https://github.com/IlyaGusev/rnnmorph, утверждалось, что оно использует тот же тегсет, но это неправда. Так или иначе, я предполагал, что от выбора здесь будеть зависеть не много, тексты вряд ли можно назвать экстравагантными с морфологической точки зрения.

### Алгоритм поиска и структура данных
Данные хранятся в трёх отедльных базах, потому что ни одна функция не обращается сразу к трём. Кажется, примерно, этот и предыдущий пункты собраны в `morpho_parsing.ipynb`, но я перестал их комментировать, потому что возвращаться после заполнения датафреймов к ним не планировал.
1. `parsed_sents.csv` хранит всё, из чего состоит предложение - множества лемм, словоформ и частей речи. Проверить, что слово в предложении просто находится очень просто и быстро, благдаря тому, что `set` хешируемый, это была главная мотивация
2. `morph_info.csv` хранит все слова в корпусе вместе с их морфохарактеристиками, в том числе словоформу, лемму, часть речи, падеж и пр. Планировалось всё из этого использовать, например, выводить всю грамматику слова в тултипах, реализация есть в `idea_testing.ipynb`, но сильно бьёт либо по памяти, либо по времени. В итоге отсюда берётся информация о слове, если его индекс уже найден, этим занимается функция `search`, её, как и другие функции для манипуляции с данными можно найти в `web_search.py`
3. `meta_sents.csv` хранит дату, источник и полный вид каждого предложения в корпусе. Именно отсюда берётся конечный результат, когда предложение триангулировано.

Таким образом, цикл такой:
1) поступает запрос
2) лезем в (1), ищем предложения, где слово нашлось, достаём его индекс и сразу же индекс слова
3) опционально: если уточнялась часть речи, то надо проверить в (2) и отфильтровать неподходящие слова
4) выводим всю инфу про предложение из (3)

Если слов в запросе несколько, то между пунктом 3 и 4 добавится пересечение двух множеств найденных предложений по общим словам. Подробности о том, как реализуется поиск, можно почитать в `web_search.py`, я добавил кое-какую документацию. Альтернативные варианты тоже рассматривались, например, поиск по одному датафрейму, где хранится и номер предложения, и номер слова. Тогда нужно было бы найти предложения при условии, что в них находится слово. Этот подход дольше реализованного почти в 3 раза, это и многое можно раскопать в `idea_testing.ipynb`. Итоговый результат работает весьма быстро, даже самые долгие запросы ищутся около 2 секунд, все операции на датафреймах векторизован, где возможно, везде, где можно, использованы хешируемые структуры, за счёт этого достигается скорость. Наверняка это неравда, и что-то где-то ускоряется ещё, но я устал. К сожалению, у всего есть своя цена, поэтому из-за такого формата датайрем размечается мучсительно медленно, у меня это заняло 7,5 часов, причём за один присест это сделать не получается, слишком много тратится памяти. Поэтому любые манипуляции с ним постфактум вообще делать опасно

Насколько я могу заметить, функция поиска везде работает корректно, я много тестил, но из-за ошибок в изначальной разметке случаются казусы. Подробнее про состав тегсета нарисовано на страничке поиска, по факту он тот же, что у `pymorphy`, но немного облегчённый, но оно и к лучшему.

### Веб-приложение
Общая структура расписана в `app.py`, там ничего особенного: титул и страница поиска. Сайт сделан по шаблонам, я не сумасшедший лепить всё это с нуля, на сайте всё это упомянуто. html я не комментировал, потому что он похож на кашу, учитывая, что у меня никак не хотели дружить flask и css и где-то что-то я писал руками прямо в шаблонах страниц. Из интересного - я реализовал вывод морфотегов, как я уже сказал, но он слишком громоздкий, а также сделал подсветку искомых слов, но она тоже громоздкая. Если запросов много, время очень сильно замедляется, поэтому на сайте она есть только для поиска по формам и только, если результатов мало.

Всё выложено (как же я с этим намучался) на `python.anywhere.com`, ссылка вверху. Функционал немного урезан, потому что приложение туда помщается впритык, например, картинок с маэстро там меньше, чем хотелось бы, но всё основное должно работать сносно. Из того, что можно улучшить тут - сделать наконец нормальные хайлайты, сделать нормальный адаптивный лэйаут и найти красивую картинку на страничку поиска. А в целом вот и всё
